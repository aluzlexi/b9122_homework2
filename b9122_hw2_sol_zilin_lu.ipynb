{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e0a70c2",
   "metadata": {},
   "source": [
    "# Question1-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab2afd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# from urllib.request import Request\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6fc91aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the link with more press releases\n",
    "seed_url = \"https://press.un.org/en\"\n",
    "try:\n",
    "    req = urllib.request.Request(seed_url,headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    webpage = urllib.request.urlopen(req).read()\n",
    "    more_url_lst = BeautifulSoup(webpage).find_all('div',{\"class\":\"more-link\"})\n",
    "    for url in more_url_lst:\n",
    "        more_url = url.find('a')['href'] \n",
    "        if 'press' in more_url and 'release' in more_url:\n",
    "            break\n",
    "except Exception as ex:\n",
    "    print(\"Unable to access the link with more press releases\")\n",
    "    print(ex) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e33e309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://press.un.org/en/content/press-release'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "more_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "800e1006",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found crisis in  https://press.un.org/en/2023/sgsm21980.doc.htm\n",
      "Found crisis in  https://press.un.org/en/2023/sgsm21978.doc.htm\n",
      "Found crisis in  https://press.un.org/en/2023/sgsm21967.doc.htm\n",
      "Found crisis in  https://press.un.org/en/2023/dsgsm1877.doc.htm\n",
      "Found crisis in  https://press.un.org/en/2023/sgsm21959.doc.htm\n",
      "Found crisis in  https://press.un.org/en/2023/sgsm21956.doc.htm\n",
      "Found crisis in  https://press.un.org/en/2023/sgsm21952.doc.htm\n",
      "Found crisis in  https://press.un.org/en/2023/sgsm21951.doc.htm\n",
      "Found crisis in  https://press.un.org/en/2023/sgsm21950.doc.htm\n",
      "Found crisis in  https://press.un.org/en/2023/sgsm21947.doc.htm\n",
      "Found crisis in  https://press.un.org/en/2023/sgsm21945.doc.htm\n"
     ]
    }
   ],
   "source": [
    "htmlCodes = []\n",
    "curr_url = more_url\n",
    "page_count = 1\n",
    "\n",
    "while len(htmlCodes) < 10:\n",
    "    \n",
    "    try:\n",
    "        req = urllib.request.Request(curr_url,headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage = urllib.request.urlopen(req).read()\n",
    "    except Exception as ex:\n",
    "        print(\"Unable to access= \"+curr_url)\n",
    "        print(ex)\n",
    "        continue\n",
    "    \n",
    "    # get all links on the page of curr_url\n",
    "    soup = BeautifulSoup(webpage)\n",
    "    for sub_url_href in soup.find_all('a', hreflang=\"en\"): \n",
    "        sub_url = \"https://press.un.org\" + sub_url_href['href']  \n",
    "        try:\n",
    "            sub_req = urllib.request.Request(sub_url,headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            sub_webpage = urllib.request.urlopen(sub_req).read()\n",
    "        except Exception as ex:\n",
    "            print(\"Unable to access= \" + sub_url)\n",
    "            print(ex)\n",
    "            continue\n",
    "            \n",
    "        sub_soup = BeautifulSoup(sub_webpage)\n",
    "        relative_anchor = sub_soup.find('a', href=\"/en/press-release\", hreflang=\"en\", string=\"Press Release\")\n",
    "        \n",
    "        if relative_anchor:\n",
    "            # check for title\n",
    "            title = sub_soup.find('h1',{\"class\":\"page-header\"})\n",
    "            if \"crisis\" in title.get_text().lower():\n",
    "                    htmlCodes.append(sub_soup)\n",
    "                    print(\"Found crisis in \", sub_url)\n",
    "            else:\n",
    "                # check for main content\n",
    "                paragraphs = sub_soup.find_all('p')\n",
    "                for paragraph in paragraphs:\n",
    "                    if \"crisis\" in paragraph.get_text().lower():\n",
    "                        htmlCodes.append(sub_soup)\n",
    "                        print(\"Found crisis in \", sub_url)\n",
    "                        break\n",
    "    \n",
    "    page_count += 1\n",
    "    curr_url = more_url + soup.find('a', title=\"Go to page \" + str(page_count))['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8a1f474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(htmlCodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06712f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_1.txt Saved\n",
      "1_2.txt Saved\n",
      "1_3.txt Saved\n",
      "1_4.txt Saved\n",
      "1_5.txt Saved\n",
      "1_6.txt Saved\n",
      "1_7.txt Saved\n",
      "1_8.txt Saved\n",
      "1_9.txt Saved\n",
      "1_10.txt Saved\n",
      "1_11.txt Saved\n"
     ]
    }
   ],
   "source": [
    "# Loop through the list of webpages\n",
    "for idx, htmlCode in enumerate(htmlCodes, start=1):\n",
    "    \n",
    "    # Construct filename based on index\n",
    "    filename = f\"1_{idx}.txt\"\n",
    "    \n",
    "    # Save the content to a file\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(htmlCode.decode('utf-8'))\n",
    "\n",
    "    print(f\"{filename} Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098c5a21",
   "metadata": {},
   "source": [
    "# Question1-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5ddcbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a398a9",
   "metadata": {},
   "source": [
    "Click 'Load more' to get more content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e750909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found crisis in  https://www.europarl.europa.eu/news/en/press-room/20230929IPR06132/nagorno-karabakh-meps-demand-review-of-eu-relations-with-azerbaijan\n",
      "Found crisis in  https://www.europarl.europa.eu/news/en/press-room/20230929IPR06130/parliament-argues-for-a-top-up-to-multi-annual-budget-for-crisis-response\n",
      "Found crisis in  https://www.europarl.europa.eu/news/en/press-room/20230911IPR04923/reduce-demand-and-protect-people-in-prostitution-say-meps\n",
      "Found crisis in  https://www.europarl.europa.eu/news/en/press-room/20230911IPR04918/svietlana-tsikhanouskaya-to-meps-support-belarusians-european-aspirations\n",
      "Found crisis in  https://www.europarl.europa.eu/news/en/press-room/20230911IPR04908/meps-vote-to-strengthen-eu-defence-industry-through-common-procurement\n",
      "Found crisis in  https://www.europarl.europa.eu/news/en/press-room/20230707IPR02427/covid-19-parliament-adopts-roadmap-to-better-prepare-for-future-health-crises\n",
      "Found crisis in  https://www.europarl.europa.eu/news/en/press-room/20230707IPR02421/parliament-adopts-new-rules-to-boost-energy-savings\n",
      "Found crisis in  https://www.europarl.europa.eu/news/en/press-room/20230707IPR02418/semiconductors-meps-adopt-legislation-to-boost-eu-chips-industry\n",
      "Found crisis in  https://www.europarl.europa.eu/news/en/press-room/20230706IPR02317/ep-today\n",
      "Found crisis in  https://www.europarl.europa.eu/news/en/press-room/20230706IPR02316/ep-today\n"
     ]
    }
   ],
   "source": [
    "htmlCodes = []\n",
    "seed_url = \"https://www.europarl.europa.eu/news/en/press-room/page/\"\n",
    "page_count = 0\n",
    "\n",
    "while len(htmlCodes) < 10:\n",
    "    \n",
    "    curr_url = seed_url + str(page_count)\n",
    "    try:\n",
    "        req = urllib.request.Request(curr_url,headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage = urllib.request.urlopen(req).read()\n",
    "    except Exception as ex:\n",
    "        print(\"Unable to access= \"+curr_url)\n",
    "        print(ex)\n",
    "        continue\n",
    "    \n",
    "    # get all links on the page of curr_url\n",
    "    soup = BeautifulSoup(webpage)\n",
    "    for sub_url_href in soup.find_all('a', title=\"Read more\"):      \n",
    "        sub_url = sub_url_href['href']   \n",
    "        try:\n",
    "            sub_req = urllib.request.Request(sub_url,headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            sub_webpage = urllib.request.urlopen(sub_req).read()\n",
    "        except Exception as ex:\n",
    "            print(\"Unable to access= \" + sub_url)\n",
    "            print(ex)\n",
    "            continue\n",
    "            \n",
    "        sub_soup = BeautifulSoup(sub_webpage)\n",
    "        relative_anchor = sub_soup.find('span', {\"class\":\"ep_name\"}, string=\"Plenary session\")\n",
    "        if relative_anchor:\n",
    "            # check for title\n",
    "            title = sub_soup.find('h1',{\"class\":\"ep_title\"})\n",
    "            if \"crisis\" in title.get_text().lower():\n",
    "                htmlCodes.append(sub_soup)\n",
    "                print(\"Found crisis in \", sub_url)\n",
    "            else:\n",
    "                # check for main content\n",
    "                paragraphs = sub_soup.find_all('p')\n",
    "                for paragraph in paragraphs:\n",
    "                    if \"crisis\" in paragraph.get_text().lower():\n",
    "                        htmlCodes.append(sub_soup)\n",
    "                        print(\"Found crisis in \", sub_url)\n",
    "                        break\n",
    "    \n",
    "    page_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "61da5587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(htmlCodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "85089334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_1.txt Saved\n",
      "2_2.txt Saved\n",
      "2_3.txt Saved\n",
      "2_4.txt Saved\n",
      "2_5.txt Saved\n",
      "2_6.txt Saved\n",
      "2_7.txt Saved\n",
      "2_8.txt Saved\n",
      "2_9.txt Saved\n",
      "2_10.txt Saved\n"
     ]
    }
   ],
   "source": [
    "# Loop through the list of webpages\n",
    "for idx, htmlCode in enumerate(htmlCodes, start=1):\n",
    "    \n",
    "    # Construct filename based on index\n",
    "    filename = f\"2_{idx}.txt\"\n",
    "    \n",
    "    # Save the content to a file\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(htmlCode.decode('utf-8'))\n",
    "\n",
    "    print(f\"{filename} Saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
